{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13877fc9-720b-4c4d-bbdb-34fe5c168e45",
   "metadata": {},
   "source": [
    "# Lab Seven: RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cafae8-8644-4ecb-98dd-75cc82f5f374",
   "metadata": {},
   "source": [
    "### Maria Harrison, Garrett Webb, Jackson Heck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf5a75-a79e-4d52-9f59-b6b5d206aca3",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46defd9e-c005-40a4-9128-b2a8d2ad4d88",
   "metadata": {},
   "source": [
    "[2 points] Define and prepare your data set. Provide details about the source of the data. Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  Also discuss your rationale for the size and nature of your vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904a8a9-a7ea-47f2-96aa-5b36f06686ef",
   "metadata": {},
   "source": [
    "We chose a small subset of a dataset of book reviews from the Amazon Kindle Store. The dataset contains 12,000 samples. Each sample contains a rating and the review text of the product. We will be performing sentiment analysis (many-to-one).\n",
    "\n",
    "Dataset source: https://www.kaggle.com/datasets/meetnagadia/amazon-kindle-book-review-for-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a06381-5b2a-4ecf-a91e-4dd0b5ae745b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "      <td>50 + years ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "      <td>Boring! Boring! Boring!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "      <td>Wiggleliscious/new toy ready/!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I don't normally buy 'mystery' novels because ...</td>\n",
       "      <td>Very good read.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This isn't the kind of book I normally read, a...</td>\n",
       "      <td>Great Story!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  rating                                         reviewText   \n",
       "0           0       5  This book was the very first bookmobile book I...  \\\n",
       "1           1       1  When I read the description for this book, I c...   \n",
       "2           2       5  I just had to edit this review. This book is a...   \n",
       "3           3       5  I don't normally buy 'mystery' novels because ...   \n",
       "4           4       5  This isn't the kind of book I normally read, a...   \n",
       "\n",
       "                           summary  \n",
       "0                50 + years ago...  \n",
       "1          Boring! Boring! Boring!  \n",
       "2  Wiggleliscious/new toy ready/!!  \n",
       "3                  Very good read.  \n",
       "4                     Great Story!  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('./preprocessed_kindle_review.csv')\n",
    "# view sample of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8f59b3-6beb-400c-b3ca-c6f5be34beb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>I don't normally buy 'mystery' novels because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This isn't the kind of book I normally read, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                         reviewText\n",
       "0       5  This book was the very first bookmobile book I...\n",
       "1       1  When I read the description for this book, I c...\n",
       "2       5  I just had to edit this review. This book is a...\n",
       "3       5  I don't normally buy 'mystery' novels because ...\n",
       "4       5  This isn't the kind of book I normally read, a..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "df = df.drop(columns=['Unnamed: 0', 'summary'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eebd28c-0afa-4177-a20f-50f42107ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos = 1, neg = 0\n",
    "df['rating'] = df['rating'].replace(to_replace=[1, 2, 3, 4, 5],value=[0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d8efc1-e792-41d3-8516-c77f159bc11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "print(df['rating'].value_counts()[0])\n",
    "print(df['rating'].value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72753c6a-3cb3-4cec-9bc7-dbd4175f57ab",
   "metadata": {},
   "source": [
    "Next, I wanted to visualize the general word count of all the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52e9d07-3302-4b40-94b9-c4769e47a25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5020 2908 3834 226 11\n"
     ]
    }
   ],
   "source": [
    "text_numpy = df.reviewText.to_numpy()\n",
    "\n",
    "num_less_than_50 = 0\n",
    "num_less_than_100 = 0\n",
    "num_less_than_500 = 0\n",
    "num_less_than_1k = 0\n",
    "num_less_than_2k = 0\n",
    "\n",
    "for i in text_numpy:\n",
    "    if(len(i.split()) < 50):\n",
    "        num_less_than_50 +=1\n",
    "    elif(len(i.split()) < 100):\n",
    "        num_less_than_100 +=1\n",
    "    elif(len(i.split()) < 500):\n",
    "        num_less_than_500 +=1\n",
    "    elif(len(i.split()) < 1000):\n",
    "        num_less_than_1k += 1\n",
    "    elif(len(i.split()) < 2000):\n",
    "        num_less_than_2k += 1\n",
    "        \n",
    "print(num_less_than_50, num_less_than_100, num_less_than_500, num_less_than_1k, num_less_than_2k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909b9a8-9421-4e9f-9fd7-659a8c8e3b94",
   "metadata": {},
   "source": [
    "It appears that most of the reviews are in the range of 0 to 500 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7273b-5836-4cb5-ad1d-d36fa93049f4",
   "metadata": {},
   "source": [
    "Discuss methods of tokenization in your dataset as well as any decisions to force a specific length of sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdbaecb-8e13-4f7b-a26f-58e08e93b111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 18:41:49.066209: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-08 18:41:49.097534: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-08 18:41:49.098031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 18:41:49.572306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33056 unique tokens. Distilled to 33056 top words.\n",
      "Shape of data tensor: (12000, 500)\n",
      "Shape of label tensor: (12000, 2)\n",
      "33056\n",
      "CPU times: user 1.77 s, sys: 1.05 s, total: 2.82 s\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = df.reviewText.to_numpy()\n",
    "y = df['rating'].to_numpy()\n",
    "\n",
    "NUM_TOP_WORDS = None\n",
    "MAX_ART_LEN = 500 # max and min num of words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(y, NUM_CLASSES)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38265a34-f918-4414-bcff-bfedbb26db9a",
   "metadata": {},
   "source": [
    "[0.5 points] Choose and explain what metric(s) you will use to evaluate your algorithm’s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c017c01-abcb-47d8-8c74-0a7ffe415a8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78581eee-079c-4aff-8ff0-e6b6a7932897",
   "metadata": {},
   "source": [
    "[0.5 points] Choose the method you will use for dividing your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your train/test splitting method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbeb8109-7a45-4450-80aa-1c7b70436b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (9600, 500) Label Shape: (9600, 2)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m uniq_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_train_ohe,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)),uniq_classes)\n\u001b[0;32m---> 13\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)), \u001b[43mclass_labels\u001b[49m, rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Instances in Each Class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_labels' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGfCAYAAABBU+jJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnq0lEQVR4nO3df1TVdYL/8ReC9/rzXvwJsqLZWiqmlrbibfsxTYzo0Iyz4ZlsHKPSWl1sRmnyx1nHX+05mjY5ZpqzldKcyYzmlJUkxmLobuKPSGYQf6wWLbh2wXThoqOg8P7+0eHz7aaSFzF44/Nxzuckn8/7fni/vV54drmfS5gxxggAAMAibZp7AgAAAKEiYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1IkIZvHDhQi1atCho34ABA3To0CFJ0rlz5/TUU09p48aNqq6uVmJiotasWaOoqChnfElJiaZNm6aPPvpInTp1UkpKipYsWaKIiP8/ldzcXKWlpamoqEixsbGaN2+eHnnkkZAWVldXp+PHj6tz584KCwsL6bYAAKB5GGNUVVWlmJgYtWnTwPMsJgQLFiwwgwcPNl9++aWznThxwjk+depUExsba3Jycswnn3xiRo0aZe644w7n+IULF8wtt9xiEhISzL59+8wHH3xgunfvbubOneuM+fzzz02HDh1MWlqaOXDggFm1apUJDw83WVlZoUzVlJaWGklsbGxsbGxsFm6lpaUNfp8PM+bKf5njwoULtWnTJhUUFFx0rLKyUj169NCGDRs0fvx4SdKhQ4c0aNAg5eXladSoUdqyZYvuv/9+HT9+3HlWZu3atZo9e7ZOnDghl8ul2bNnKzMzU/v373fOPWHCBFVUVCgrK+tKp6rKykpFRkaqtLRUHo/nim8HAACaTyAQUGxsrCoqKuT1ei87LqQfIUnSkSNHFBMTo3bt2snn82nJkiXq06eP8vPzdf78eSUkJDhjBw4cqD59+jgBk5eXpyFDhgT9SCkxMVHTpk1TUVGRbrvtNuXl5QWdo37MjBkzGpxXdXW1qqurnY+rqqokSR6Ph4ABAMAy3/Xyj5BexBsfH6/09HRlZWXppZdeUnFxse666y5VVVXJ7/fL5XIpMjIy6DZRUVHy+/2SJL/fHxQv9cfrjzU0JhAI6OzZs5ed25IlS+T1ep0tNjY2lKUBAACLhPQMzNixY50/Dx06VPHx8erbt68yMjLUvn37Jp9cKObOnau0tDTn4/qnoAAAQOtzVZdRR0ZG6uabb9bRo0cVHR2tmpoaVVRUBI0pKytTdHS0JCk6OlplZWUXHa8/1tAYj8fTYCS53W7nx0X82AgAgNbtqgLm9OnT+uyzz9SrVy+NGDFCbdu2VU5OjnP88OHDKikpkc/nkyT5fD4VFhaqvLzcGZOdnS2Px6O4uDhnzDfPUT+m/hwAAAAhBcxvfvMbbd++XV988YV27typf/qnf1J4eLgeeugheb1eTZ48WWlpafroo4+Un5+vRx99VD6fT6NGjZIkjR49WnFxcZo0aZL+8pe/aOvWrZo3b55SU1PldrslSVOnTtXnn3+uWbNm6dChQ1qzZo0yMjI0c+bMpl89AACwUkivgTl27JgeeughnTx5Uj169NCdd96pXbt2qUePHpKkFStWqE2bNkpOTg56I7t64eHh2rx5s6ZNmyafz6eOHTsqJSVFixcvdsb069dPmZmZmjlzplauXKnevXvrlVdeUWJiYhMtGQAA2C6k94GxSSAQkNfrVWVlJa+HAQDAElf6/ZvfhQQAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOiG9kR2+dsOczOaeAtCifbE0qbmn0CR4rAOX19yPc56BAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1ripgli5dqrCwMM2YMcPZd+7cOaWmpqpbt27q1KmTkpOTVVZWFnS7kpISJSUlqUOHDurZs6eefvppXbhwIWhMbm6uhg8fLrfbrf79+ys9Pf1qpgoAAFqRRgfM3r179Yc//EFDhw4N2j9z5ky9//77euutt7R9+3YdP35cDzzwgHO8trZWSUlJqqmp0c6dO/Xaa68pPT1d8+fPd8YUFxcrKSlJ9957rwoKCjRjxgxNmTJFW7dubex0AQBAK9KogDl9+rQmTpyol19+WV26dHH2V1ZW6tVXX9Xzzz+vH/7whxoxYoTWr1+vnTt3ateuXZKkDz/8UAcOHNCf/vQn3XrrrRo7dqyeeeYZrV69WjU1NZKktWvXql+/fvrd736nQYMGafr06Ro/frxWrFjRBEsGAAC2a1TApKamKikpSQkJCUH78/Pzdf78+aD9AwcOVJ8+fZSXlydJysvL05AhQxQVFeWMSUxMVCAQUFFRkTPm2+dOTEx0znEp1dXVCgQCQRsAAGidIkK9wcaNG/Xpp59q7969Fx3z+/1yuVyKjIwM2h8VFSW/3++M+Wa81B+vP9bQmEAgoLNnz6p9+/YXfe4lS5Zo0aJFoS4HAABYKKRnYEpLS/XrX/9ar7/+utq1a3et5tQoc+fOVWVlpbOVlpY295QAAMA1ElLA5Ofnq7y8XMOHD1dERIQiIiK0fft2vfDCC4qIiFBUVJRqampUUVERdLuysjJFR0dLkqKjoy+6Kqn+4+8a4/F4LvnsiyS53W55PJ6gDQAAtE4hBcx9992nwsJCFRQUONvtt9+uiRMnOn9u27atcnJynNscPnxYJSUl8vl8kiSfz6fCwkKVl5c7Y7Kzs+XxeBQXF+eM+eY56sfUnwMAAFzfQnoNTOfOnXXLLbcE7evYsaO6devm7J88ebLS0tLUtWtXeTwePfnkk/L5fBo1apQkafTo0YqLi9OkSZO0bNky+f1+zZs3T6mpqXK73ZKkqVOn6sUXX9SsWbP02GOPadu2bcrIyFBmZmZTrBkAAFgu5BfxfpcVK1aoTZs2Sk5OVnV1tRITE7VmzRrneHh4uDZv3qxp06bJ5/OpY8eOSklJ0eLFi50x/fr1U2ZmpmbOnKmVK1eqd+/eeuWVV5SYmNjU0wUAABYKM8aY5p7EtRAIBOT1elVZWdnkr4e5YQ7PBAEN+WJpUnNPoUnwWAcu71o9zq/0+ze/CwkAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHVCCpiXXnpJQ4cOlcfjkcfjkc/n05YtW5zj586dU2pqqrp166ZOnTopOTlZZWVlQecoKSlRUlKSOnTooJ49e+rpp5/WhQsXgsbk5uZq+PDhcrvd6t+/v9LT0xu/QgAA0OqEFDC9e/fW0qVLlZ+fr08++UQ//OEPNW7cOBUVFUmSZs6cqffff19vvfWWtm/fruPHj+uBBx5wbl9bW6ukpCTV1NRo586deu2115Senq758+c7Y4qLi5WUlKR7771XBQUFmjFjhqZMmaKtW7c20ZIBAIDtwowx5mpO0LVrVy1fvlzjx49Xjx49tGHDBo0fP16SdOjQIQ0aNEh5eXkaNWqUtmzZovvvv1/Hjx9XVFSUJGnt2rWaPXu2Tpw4IZfLpdmzZyszM1P79+93PseECRNUUVGhrKysy86jurpa1dXVzseBQECxsbGqrKyUx+O5miVe5IY5mU16PqC1+WJpUnNPoUnwWAcu71o9zgOBgLxe73d+/270a2Bqa2u1ceNGnTlzRj6fT/n5+Tp//rwSEhKcMQMHDlSfPn2Ul5cnScrLy9OQIUOceJGkxMREBQIB51mcvLy8oHPUj6k/x+UsWbJEXq/X2WJjYxu7NAAA0MKFHDCFhYXq1KmT3G63pk6dqnfeeUdxcXHy+/1yuVyKjIwMGh8VFSW/3y9J8vv9QfFSf7z+WENjAoGAzp49e9l5zZ07V5WVlc5WWloa6tIAAIAlIkK9wYABA1RQUKDKykr9+c9/VkpKirZv334t5hYSt9stt9vd3NMAAADfg5ADxuVyqX///pKkESNGaO/evVq5cqUefPBB1dTUqKKiIuhZmLKyMkVHR0uSoqOjtWfPnqDz1V+l9M0x375yqaysTB6PR+3btw91ugAAoBW66veBqaurU3V1tUaMGKG2bdsqJyfHOXb48GGVlJTI5/NJknw+nwoLC1VeXu6Myc7OlsfjUVxcnDPmm+eoH1N/DgAAgJCegZk7d67Gjh2rPn36qKqqShs2bFBubq62bt0qr9eryZMnKy0tTV27dpXH49GTTz4pn8+nUaNGSZJGjx6tuLg4TZo0ScuWLZPf79e8efOUmprq/Phn6tSpevHFFzVr1iw99thj2rZtmzIyMpSZydUAAADgayEFTHl5uR5++GF9+eWX8nq9Gjp0qLZu3aof/ehHkqQVK1aoTZs2Sk5OVnV1tRITE7VmzRrn9uHh4dq8ebOmTZsmn8+njh07KiUlRYsXL3bG9OvXT5mZmZo5c6ZWrlyp3r1765VXXlFiYmITLRkAANjuqt8HpqW60uvIG4P3hgAaxvvAAK2fte8DAwAA0FwIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYJ2QAmbJkiX6h3/4B3Xu3Fk9e/bUz372Mx0+fDhozLlz55Samqpu3bqpU6dOSk5OVllZWdCYkpISJSUlqUOHDurZs6eefvppXbhwIWhMbm6uhg8fLrfbrf79+ys9Pb1xKwQAAK1OSAGzfft2paamateuXcrOztb58+c1evRonTlzxhkzc+ZMvf/++3rrrbe0fft2HT9+XA888IBzvLa2VklJSaqpqdHOnTv12muvKT09XfPnz3fGFBcXKykpSffee68KCgo0Y8YMTZkyRVu3bm2CJQMAANuFGWNMY2984sQJ9ezZU9u3b9fdd9+tyspK9ejRQxs2bND48eMlSYcOHdKgQYOUl5enUaNGacuWLbr//vt1/PhxRUVFSZLWrl2r2bNn68SJE3K5XJo9e7YyMzO1f/9+53NNmDBBFRUVysrKuqK5BQIBeb1eVVZWyuPxNHaJl3TDnMwmPR/Q2nyxNKm5p9AkeKwDl3etHudX+v37ql4DU1lZKUnq2rWrJCk/P1/nz59XQkKCM2bgwIHq06eP8vLyJEl5eXkaMmSIEy+SlJiYqEAgoKKiImfMN89RP6b+HJdSXV2tQCAQtAEAgNap0QFTV1enGTNm6B//8R91yy23SJL8fr9cLpciIyODxkZFRcnv9ztjvhkv9cfrjzU0JhAI6OzZs5ecz5IlS+T1ep0tNja2sUsDAAAtXKMDJjU1Vfv379fGjRubcj6NNnfuXFVWVjpbaWlpc08JAABcIxGNudH06dO1efNm7dixQ71793b2R0dHq6amRhUVFUHPwpSVlSk6OtoZs2fPnqDz1V+l9M0x375yqaysTB6PR+3bt7/knNxut9xud2OWAwAALBPSMzDGGE2fPl3vvPOOtm3bpn79+gUdHzFihNq2baucnBxn3+HDh1VSUiKfzydJ8vl8KiwsVHl5uTMmOztbHo9HcXFxzphvnqN+TP05AADA9S2kZ2BSU1O1YcMGvfvuu+rcubPzmhWv16v27dvL6/Vq8uTJSktLU9euXeXxePTkk0/K5/Np1KhRkqTRo0crLi5OkyZN0rJly+T3+zVv3jylpqY6z6BMnTpVL774ombNmqXHHntM27ZtU0ZGhjIzuSIAAACE+AzMSy+9pMrKSv3gBz9Qr169nO3NN990xqxYsUL333+/kpOTdffddys6Olpvv/22czw8PFybN29WeHi4fD6ffvnLX+rhhx/W4sWLnTH9+vVTZmamsrOzNWzYMP3ud7/TK6+8osTExCZYMgAAsN1VvQ9MS8b7wADNh/eBAVo/q98HBgAAoDkQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDohB8yOHTv0k5/8RDExMQoLC9OmTZuCjhtjNH/+fPXq1Uvt27dXQkKCjhw5EjTm1KlTmjhxojwejyIjIzV58mSdPn06aMxf//pX3XXXXWrXrp1iY2O1bNmy0FcHAABapZAD5syZMxo2bJhWr159yePLli3TCy+8oLVr12r37t3q2LGjEhMTde7cOWfMxIkTVVRUpOzsbG3evFk7duzQE0884RwPBAIaPXq0+vbtq/z8fC1fvlwLFy7Uv//7vzdiiQAAoLWJCPUGY8eO1dixYy95zBij3//+95o3b57GjRsnSfrjH/+oqKgobdq0SRMmTNDBgweVlZWlvXv36vbbb5ckrVq1Sj/+8Y/13HPPKSYmRq+//rpqamq0bt06uVwuDR48WAUFBXr++eeDQgcAAFyfmvQ1MMXFxfL7/UpISHD2eb1excfHKy8vT5KUl5enyMhIJ14kKSEhQW3atNHu3budMXfffbdcLpczJjExUYcPH9b//d//XfJzV1dXKxAIBG0AAKB1atKA8fv9kqSoqKig/VFRUc4xv9+vnj17Bh2PiIhQ165dg8Zc6hzf/BzftmTJEnm9XmeLjY29+gUBAIAWqdVchTR37lxVVlY6W2lpaXNPCQAAXCNNGjDR0dGSpLKysqD9ZWVlzrHo6GiVl5cHHb9w4YJOnToVNOZS5/jm5/g2t9stj8cTtAEAgNapSQOmX79+io6OVk5OjrMvEAho9+7d8vl8kiSfz6eKigrl5+c7Y7Zt26a6ujrFx8c7Y3bs2KHz5887Y7KzszVgwAB16dKlKacMAAAsFHLAnD59WgUFBSooKJD09Qt3CwoKVFJSorCwMM2YMUP/9m//pvfee0+FhYV6+OGHFRMTo5/97GeSpEGDBmnMmDF6/PHHtWfPHn388ceaPn26JkyYoJiYGEnSL37xC7lcLk2ePFlFRUV68803tXLlSqWlpTXZwgEAgL1Cvoz6k08+0b333ut8XB8VKSkpSk9P16xZs3TmzBk98cQTqqio0J133qmsrCy1a9fOuc3rr7+u6dOn67777lObNm2UnJysF154wTnu9Xr14YcfKjU1VSNGjFD37t01f/58LqEGAACSpDBjjGnuSVwLgUBAXq9XlZWVTf56mBvmZDbp+YDW5oulSc09hSbBYx24vGv1OL/S79+t5iokAABw/SBgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgnRYdMKtXr9YNN9ygdu3aKT4+Xnv27GnuKQEAgBagxQbMm2++qbS0NC1YsECffvqphg0bpsTERJWXlzf31AAAQDOLaO4JXM7zzz+vxx9/XI8++qgkae3atcrMzNS6des0Z86ci8ZXV1erurra+biyslKSFAgEmnxuddV/a/JzAq3JtXjcNQce68DlXavHef15jTENDzQtUHV1tQkPDzfvvPNO0P6HH37Y/PSnP73kbRYsWGAksbGxsbGxsbWCrbS0tMFWaJHPwHz11Veqra1VVFRU0P6oqCgdOnTokreZO3eu0tLSnI/r6up06tQpdevWTWFhYdd0vi1BIBBQbGysSktL5fF4mns636vrde3X67ql63ft1+u6JdZ+Pa3dGKOqqirFxMQ0OK5FBkxjuN1uud3uoH2RkZHNM5lm5PF4rot/4Jdyva79el23dP2u/Xpdt8Tar5e1e73e7xzTIl/E2717d4WHh6usrCxof1lZmaKjo5tpVgAAoKVokQHjcrk0YsQI5eTkOPvq6uqUk5Mjn8/XjDMDAAAtQYv9EVJaWppSUlJ0++23a+TIkfr973+vM2fOOFclIZjb7daCBQsu+jHa9eB6Xfv1um7p+l379bpuibVfr2tvSJgx33WdUvN58cUXtXz5cvn9ft1666164YUXFB8f39zTAgAAzaxFBwwAAMCltMjXwAAAADSEgAEAANYhYAAAgHUIGAAAYB0CxhKnTp3SxIkT5fF4FBkZqcmTJ+v06dMNjn/yySc1YMAAtW/fXn369NGvfvUr55dc1gsLC7to27hx47VeToNWr16tG264Qe3atVN8fLz27NnT4Pi33npLAwcOVLt27TRkyBB98MEHQceNMZo/f7569eql9u3bKyEhQUeOHLmWS2i0UNb+8ssv66677lKXLl3UpUsXJSQkXDT+kUceuej+HTNmzLVeRshCWXd6evpFa2rXrl3QmNZ6n//gBz+45GM2KSnJGWPDfb5jxw795Cc/UUxMjMLCwrRp06bvvE1ubq6GDx8ut9ut/v37Kz09/aIxoX7taA6hrv3tt9/Wj370I/Xo0UMej0c+n09bt24NGrNw4cKL7vOBAwdew1W0EFf9mxfxvRgzZowZNmyY2bVrl/nP//xP079/f/PQQw9ddnxhYaF54IEHzHvvvWeOHj1qcnJyzE033WSSk5ODxkky69evN19++aWznT179lov57I2btxoXC6XWbdunSkqKjKPP/64iYyMNGVlZZcc//HHH5vw8HCzbNkyc+DAATNv3jzTtm1bU1hY6IxZunSp8Xq9ZtOmTeYvf/mL+elPf2r69evXrOu8lFDX/otf/MKsXr3a7Nu3zxw8eNA88sgjxuv1mmPHjjljUlJSzJgxY4Lu31OnTn1fS7oioa57/fr1xuPxBK3J7/cHjWmt9/nJkyeD1r1//34THh5u1q9f74yx4T7/4IMPzL/+67+at99+20i66Bf3ftvnn39uOnToYNLS0syBAwfMqlWrTHh4uMnKynLGhPp32VxCXfuvf/1r8+yzz5o9e/aY//7v/zZz5841bdu2NZ9++qkzZsGCBWbw4MFB9/mJEyeu8UqaHwFjgQMHDhhJZu/evc6+LVu2mLCwMPO///u/V3yejIwM43K5zPnz5519V/IA+j6NHDnSpKamOh/X1taamJgYs2TJkkuO//nPf26SkpKC9sXHx5t//ud/NsYYU1dXZ6Kjo83y5cud4xUVFcbtdps33njjGqyg8UJd+7dduHDBdO7c2bz22mvOvpSUFDNu3LimnmqTCnXd69evN16v97Lnu57u8xUrVpjOnTub06dPO/tsuM+/6Uq+Bs2aNcsMHjw4aN+DDz5oEhMTnY+v9u+yOTT2629cXJxZtGiR8/GCBQvMsGHDmm5iluBHSBbIy8tTZGSkbr/9dmdfQkKC2rRpo927d1/xeSorK+XxeBQREfwGzKmpqerevbtGjhypdevWyTTTWwPV1NQoPz9fCQkJzr42bdooISFBeXl5l7xNXl5e0HhJSkxMdMYXFxfL7/cHjfF6vYqPj7/sOZtDY9b+bX/72990/vx5de3aNWh/bm6uevbsqQEDBmjatGk6efJkk879ajR23adPn1bfvn0VGxurcePGqaioyDl2Pd3nr776qiZMmKCOHTsG7W/J93ljfNfjvCn+Lm1RV1enqqqqix7nR44cUUxMjG688UZNnDhRJSUlzTTD7w8BYwG/36+ePXsG7YuIiFDXrl3l9/uv6BxfffWVnnnmGT3xxBNB+xcvXqyMjAxlZ2crOTlZ//Iv/6JVq1Y12dxD8dVXX6m2tlZRUVFB+6Oioi67Tr/f3+D4+v+Gcs7m0Ji1f9vs2bMVExMT9EV8zJgx+uMf/6icnBw9++yz2r59u8aOHava2tomnX9jNWbdAwYM0Lp16/Tuu+/qT3/6k+rq6nTHHXfo2LFjkq6f+3zPnj3av3+/pkyZErS/pd/njXG5x3kgENDZs2eb5PFji+eee06nT5/Wz3/+c2dffHy80tPTlZWVpZdeeknFxcW66667VFVV1YwzvfZa7O9Cuh7MmTNHzz77bINjDh48eNWfJxAIKCkpSXFxcVq4cGHQsd/+9rfOn2+77TadOXNGy5cv169+9aur/rz4/ixdulQbN25Ubm5u0AtaJ0yY4Px5yJAhGjp0qP7+7/9eubm5uu+++5pjqlfN5/MF/VLXO+64Q4MGDdIf/vAHPfPMM804s+/Xq6++qiFDhmjkyJFB+1vjfY6vbdiwQYsWLdK7774b9D+1Y8eOdf48dOhQxcfHq2/fvsrIyNDkyZObY6rfC56BaUZPPfWUDh482OB24403Kjo6WuXl5UG3vXDhgk6dOqXo6OgGP0dVVZXGjBmjzp0765133lHbtm0bHB8fH69jx46purr6qtcXqu7duys8PFxlZWVB+8vKyi67zujo6AbH1/83lHM2h8asvd5zzz2npUuX6sMPP9TQoUMbHHvjjTeqe/fuOnr06FXPuSlczbrrtW3bVrfddpuzpuvhPj9z5ow2btx4Rd+cWtp93hiXe5x7PB61b9++Sf4dtXQbN27UlClTlJGRcdGP074tMjJSN998s9X3+ZUgYJpRjx49NHDgwAY3l8sln8+niooK5efnO7fdtm2b6urqGvzlloFAQKNHj5bL5dJ777130aWml1JQUKAuXbo0y289dblcGjFihHJycpx9dXV1ysnJCfo/7m/y+XxB4yUpOzvbGd+vXz9FR0cHjQkEAtq9e/dlz9kcGrN2SVq2bJmeeeYZZWVlBb1G6nKOHTumkydPqlevXk0y76vV2HV/U21trQoLC501tfb7XPr6rQOqq6v1y1/+8js/T0u7zxvjux7nTfHvqCV744039Oijj+qNN94IumT+ck6fPq3PPvvM6vv8ijT3q4hxZcaMGWNuu+02s3v3bvNf//Vf5qabbgq6jPrYsWNmwIABZvfu3cYYYyorK018fLwZMmSIOXr0aNDldRcuXDDGGPPee++Zl19+2RQWFpojR46YNWvWmA4dOpj58+c3yxqN+fpSSLfbbdLT082BAwfME088YSIjI53LZCdNmmTmzJnjjP/4449NRESEee6558zBgwfNggULLnkZdWRkpHn33XfNX//6VzNu3LgWe0ltKGtfunSpcblc5s9//nPQ/VtVVWWMMaaqqsr85je/MXl5eaa4uNj8x3/8hxk+fLi56aabzLlz55pljZcS6roXLVpktm7daj777DOTn59vJkyYYNq1a2eKioqcMa31Pq935513mgcffPCi/bbc51VVVWbfvn1m3759RpJ5/vnnzb59+8z//M//GGOMmTNnjpk0aZIzvv4y6qefftocPHjQrF69+pKXUTf0d9lShLr2119/3URERJjVq1cHPc4rKiqcMU899ZTJzc01xcXF5uOPPzYJCQmme/fupry8/Htf3/eJgLHEyZMnzUMPPWQ6depkPB6PefTRR51vVMYYU1xcbCSZjz76yBhjzEcffWQkXXIrLi42xnx9Kfatt95qOnXqZDp27GiGDRtm1q5da2pra5thhf/fqlWrTJ8+fYzL5TIjR440u3btco7dc889JiUlJWh8RkaGufnmm43L5TKDBw82mZmZQcfr6urMb3/7WxMVFWXcbre57777zOHDh7+PpYQslLX37dv3kvfvggULjDHG/O1vfzOjR482PXr0MG3btjV9+/Y1jz/+eIv7gm5MaOueMWOGMzYqKsr8+Mc/DnpPDGNa731ujDGHDh0yksyHH3540blsuc8v9/Wpfq0pKSnmnnvuueg2t956q3G5XObGG28Meu+beg39XbYUoa79nnvuaXC8MV9fUt6rVy/jcrnM3/3d35kHH3zQHD169PtdWDMIM6aZrpkFAABoJF4DAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDr/D6Q51eblstElAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=y)\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train_ohe.shape)\n",
    "uniq_classes = np.sum(y_train_ohe,axis=0)\n",
    "plt.bar(list(range(2)),uniq_classes)\n",
    "plt.xticks(list(range(2)), class_labels, rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d193c29e-72dc-4daf-980d-430451d105ed",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a44fb9-b5cf-49ae-8470-e88bbb532721",
   "metadata": {},
   "source": [
    "[2 points] Investigate at least two different recurrent network architectures  Be sure to use an embedding layer . Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them. Justify your choice of parameters for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0b80eed-935c-4ca3-981b-35ac6721ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Embedding Shape: (33057, 200) \n",
      " Total words found: 26163 \n",
      " Percentage: 79.14511298665941\n",
      "CPU times: user 16.6 s, sys: 254 ms, total: 16.8 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 200\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('./glove/glove.6B.200d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ab77686-127a-45c2-a237-ce187d8d4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# save this embedding now\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38a66e24-ec75-494c-bd0b-df9b41c15b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b3cb2d9-2835-44b0-8300-8422fb3037cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 17:34:34.308116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-08 17:34:34.308488: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(embedding_layer)\n",
    "lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=.2))\n",
    "lstm.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "lstm.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "gru = Sequential()\n",
    "gru.add(embedding_layer)\n",
    "gru.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "gru.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "gru.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'rmsprop', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff26c9eb-bda3-46b5-b350-04e9c8b85145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "150/150 [==============================] - 34s 219ms/step - loss: 0.5974 - accuracy: 0.6796 - val_loss: 0.5545 - val_accuracy: 0.7175\n",
      "Epoch 2/6\n",
      "150/150 [==============================] - 32s 215ms/step - loss: 0.5258 - accuracy: 0.7482 - val_loss: 0.4875 - val_accuracy: 0.7771\n",
      "Epoch 3/6\n",
      "150/150 [==============================] - 33s 217ms/step - loss: 0.5133 - accuracy: 0.7566 - val_loss: 0.4716 - val_accuracy: 0.7817\n",
      "Epoch 4/6\n",
      "150/150 [==============================] - 32s 216ms/step - loss: 0.4798 - accuracy: 0.7808 - val_loss: 0.4564 - val_accuracy: 0.7846\n",
      "Epoch 5/6\n",
      "150/150 [==============================] - 32s 215ms/step - loss: 0.4558 - accuracy: 0.7918 - val_loss: 0.4481 - val_accuracy: 0.7933\n",
      "Epoch 6/6\n",
      "150/150 [==============================] - 32s 215ms/step - loss: 0.4375 - accuracy: 0.8031 - val_loss: 0.4527 - val_accuracy: 0.7887\n"
     ]
    }
   ],
   "source": [
    "history_lstm = []\n",
    "tmp = lstm.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history_lstm.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f762b4d2-801e-4535-82f9-0f0a0e719854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "150/150 [==============================] - 30s 193ms/step - loss: 0.6170 - accuracy: 0.6560 - val_loss: 0.4955 - val_accuracy: 0.7679\n",
      "Epoch 2/6\n",
      "150/150 [==============================] - 29s 193ms/step - loss: 0.5095 - accuracy: 0.7582 - val_loss: 0.4676 - val_accuracy: 0.7850\n",
      "Epoch 3/6\n",
      "150/150 [==============================] - 29s 193ms/step - loss: 0.4691 - accuracy: 0.7840 - val_loss: 0.4325 - val_accuracy: 0.8079\n",
      "Epoch 4/6\n",
      "150/150 [==============================] - 29s 192ms/step - loss: 0.4390 - accuracy: 0.8054 - val_loss: 0.4257 - val_accuracy: 0.8138\n",
      "Epoch 5/6\n",
      "150/150 [==============================] - 29s 190ms/step - loss: 0.4170 - accuracy: 0.8143 - val_loss: 0.4144 - val_accuracy: 0.8150\n",
      "Epoch 6/6\n",
      "150/150 [==============================] - 29s 193ms/step - loss: 0.4006 - accuracy: 0.8257 - val_loss: 0.4308 - val_accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "history_gru = []\n",
    "tmp = gru.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history_gru.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9027ce6-2926-4465-a3df-e4fc0f3d6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2 = Sequential()\n",
    "lstm_2.add(embedding_layer)\n",
    "lstm_2.add(LSTM(100, dropout=0.2, recurrent_dropout=.2))\n",
    "lstm_2.add(Dense(NUM_CLASSES, activation='relu'))\n",
    "lstm_2.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "gru_2 = Sequential()\n",
    "gru_2.add(embedding_layer)\n",
    "gru_2.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "gru_2.add(Dense(NUM_CLASSES, activation='relu'))\n",
    "gru_2.compile(loss='binary_crossentropy', \n",
    "              optimizer= 'adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b131e50d-d412-4eb3-a5ba-3ed7a16c3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "150/150 [==============================] - 34s 218ms/step - loss: 0.7967 - accuracy: 0.6053 - val_loss: 0.6383 - val_accuracy: 0.6604\n",
      "Epoch 2/6\n",
      "150/150 [==============================] - 32s 216ms/step - loss: 0.6232 - accuracy: 0.6842 - val_loss: 0.5798 - val_accuracy: 0.7192\n",
      "Epoch 3/6\n",
      "150/150 [==============================] - 32s 216ms/step - loss: 0.5895 - accuracy: 0.7297 - val_loss: 0.6192 - val_accuracy: 0.6558\n",
      "Epoch 4/6\n",
      "150/150 [==============================] - 33s 218ms/step - loss: 0.5461 - accuracy: 0.7526 - val_loss: 0.5077 - val_accuracy: 0.7842\n",
      "Epoch 5/6\n",
      "150/150 [==============================] - 33s 217ms/step - loss: 0.5146 - accuracy: 0.7800 - val_loss: 0.5134 - val_accuracy: 0.7904\n",
      "Epoch 6/6\n",
      "150/150 [==============================] - 33s 217ms/step - loss: 0.5025 - accuracy: 0.7879 - val_loss: 0.4655 - val_accuracy: 0.8012\n"
     ]
    }
   ],
   "source": [
    "history_lstm_2 = []\n",
    "tmp = lstm_2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history_lstm_2.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b467689-7914-490a-856e-8a685e5d485f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "150/150 [==============================] - 30s 195ms/step - loss: 1.3652 - accuracy: 0.5576 - val_loss: 0.8556 - val_accuracy: 0.6304\n",
      "Epoch 2/6\n",
      "150/150 [==============================] - 29s 193ms/step - loss: 3.4880 - accuracy: 0.5211 - val_loss: 4.1418 - val_accuracy: 0.5004\n",
      "Epoch 3/6\n",
      "150/150 [==============================] - 29s 195ms/step - loss: 4.1450 - accuracy: 0.4999 - val_loss: 4.1200 - val_accuracy: 0.5000\n",
      "Epoch 4/6\n",
      "150/150 [==============================] - 29s 193ms/step - loss: 4.0996 - accuracy: 0.5004 - val_loss: 4.0840 - val_accuracy: 0.5000\n",
      "Epoch 5/6\n",
      "150/150 [==============================] - 29s 195ms/step - loss: 3.9861 - accuracy: 0.5025 - val_loss: 3.9924 - val_accuracy: 0.5000\n",
      "Epoch 6/6\n",
      "150/150 [==============================] - 29s 194ms/step - loss: 2.5882 - accuracy: 0.5434 - val_loss: 1.4606 - val_accuracy: 0.5858\n"
     ]
    }
   ],
   "source": [
    "history_gru_2 = []\n",
    "tmp = gru_2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history_gru_2.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe76bbc-7325-47a7-9cd1-14655c515818",
   "metadata": {},
   "source": [
    "[1 points] Using the best parameters and architecture from the RNN in the previous step, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34fdc5b4-3bcb-41c6-a0a8-225e865484ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's extend the training by a number of epochs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_ohe, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test_ohe), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      3\u001b[0m history\u001b[38;5;241m.\u001b[39mappend( tmp )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnn' is not defined"
     ]
    }
   ],
   "source": [
    "# let's extend the training by a number of epochs\n",
    "tmp = rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f60b82-dc73-4253-abda-93f940437333",
   "metadata": {},
   "source": [
    "[0.5 points] Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a966d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['accuracy','val_accuracy','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in history])\n",
    "    \n",
    "plot_history(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d636f93-979f-4c4b-89e5-6674f383f89c",
   "metadata": {},
   "source": [
    "[0.5 points] Run to convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066812c-f676-4781-962e-8f698f627939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c23601d-b3a6-422e-b50a-d2948bb1055b",
   "metadata": {},
   "source": [
    "[1 point]  Visualize the results of all the RNNs you trained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cdbdb3-dff7-4e02-b4d5-496055247d89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a1f8645-9f14-416c-b001-de9a42b45aeb",
   "metadata": {},
   "source": [
    "## 3. Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e15783-116b-4b5a-992e-994cc1c555d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import re\n",
    "#import statsmodels.formula.api\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configure how graphs will show up in this notebook\n",
    "%matplotlib inline\n",
    "seaborn.set_context('notebook', rc={'figure.figsize': (10, 6)}, font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e157ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2196017, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('./glove/glove.840B.300d.txt')\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9cfaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783\n"
     ]
    }
   ],
   "source": [
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('./conceptnet/positive-words.txt')\n",
    "neg_words = load_lexicon('./conceptnet/negative-words.txt')\n",
    "\n",
    "print(len(pos_words), len(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bfd4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 300) (4642, 300)\n"
     ]
    }
   ],
   "source": [
    "pos_words_common = list(set(pos_words) & set(embeddings.index)) \n",
    "neg_words_common = list(set(neg_words) & set(embeddings.index)) \n",
    "\n",
    "pos_vectors = embeddings.loc[pos_words_common]\n",
    "neg_vectors = embeddings.loc[neg_words_common]\n",
    "print(pos_vectors.shape,neg_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6410b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e617c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4c6c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpheckles/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log&#x27;, max_iter=100, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log&#x27;, max_iter=100, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log', max_iter=100, random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a linear classifier \n",
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4083033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9546827794561934"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model.predict(test_vectors), test_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f06a184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    # this is a logarithm of the confidence margin for the classifier\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs= embeddings.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "# words_to_sentiment(test_labels).iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7555394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    # tokenize the input phrase\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    # send each token separately into the embedding, then the classifier\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean() # return the mean for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2214302c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['bechett', 'desaxby', 'mfff'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./preprocessed_kindle_review.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     guesses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_to_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreviewText\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m guesses\n",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m, in \u001b[0;36mtext_to_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mcasefold() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m TOKEN_RE\u001b[38;5;241m.\u001b[39mfindall(text)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# send each token separately into the embedding, then the classifier\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[43mwords_to_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[34], line 13\u001b[0m, in \u001b[0;36mwords_to_sentiment\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords_to_sentiment\u001b[39m(words):\n\u001b[0;32m---> 13\u001b[0m     vecs\u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     14\u001b[0m     log_odds \u001b[38;5;241m=\u001b[39m vecs_to_sentiment(vecs)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m: log_odds}, index\u001b[38;5;241m=\u001b[39mvecs\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5876\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5878\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5880\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5937\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['bechett', 'desaxby', 'mfff'] not in index\""
     ]
    }
   ],
   "source": [
    "guesses = []\n",
    "df = pd.read_csv('./preprocessed_kindle_review.csv')\n",
    "for i in range(3):\n",
    "    guesses.append(text_to_sentiment(df.reviewText[i]))\n",
    "guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2719ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67f95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fcc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
